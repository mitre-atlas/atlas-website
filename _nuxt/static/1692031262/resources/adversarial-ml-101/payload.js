__NUXT_JSONP__("/resources/adversarial-ml-101", (function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O){return {data:[{content:{slug:"adversarial-ml-101",toc:[{id:x,depth:o,text:y},{id:z,depth:p,text:A},{id:B,depth:p,text:C},{id:D,depth:o,text:E},{id:F,depth:o,text:G},{id:H,depth:o,text:I},{id:J,depth:p,text:K},{id:L,depth:p,text:M}],body:{type:"root",children:[{type:a,tag:e,props:{},children:[{type:b,value:"The methods underpinning the production machine learning systems are\nsystematically vulnerable to a new class of vulnerabilities across the\nmachine learning supply chain collectively known as Adversarial Machine\nLearning. Adversaries can exploit these vulnerabilities to manipulate AI\nsystems in order to alter their behavior to serve a malicious end goal."}]},{type:b,value:c},{type:a,tag:e,props:{},children:[{type:b,value:"Consider a typical ML pipeline shown below that is gated behind an API,\nwherein the only way to use the model is to send a query and observe a\nresponse. In this example, we assume a blackbox setting: the attacker does\nNOT have direct access to the training data, no knowledge of the algorithm\nused and no source code of the model. The attacker only queries the model\nand observes the response. We will look at two broad categories of\nattacks:"}]},{type:b,value:c},{type:a,tag:q,props:{id:x},children:[{type:a,tag:g,props:{href:"#train-time-vs-inference-time",ariaHidden:h,tabIndex:i},children:[{type:a,tag:j,props:{className:[k,l]},children:[]}]},{type:b,value:y}]},{type:b,value:c},{type:a,tag:e,props:{},children:[{type:b,value:"Training refers to the process by which data is modeled. This process\nincludes collecting and processing data, training a model, validating the\nmodel works, and then finally deploying the model. An attack that happens\nat \"train time\" is an attack that happens while the model is learning\nprior its deployment. After a model is deployed, consumers of the model\ncan submit queries and receive outputs (inferences). An attack that\nhappens at \"inference time\" is an attack where the learned state of the\nmodel does not change and the model is just providing outputs. In\npractice, a model could be re-trained after every new query providing an\nattacker with some interesting scenarios by which they could use an\ninference endpoint to perform a \"train-time\" attack. In any case, the\ndelineation is useful to describe how an attacker could be interacting\nwith a target model."}]},{type:b,value:c},{type:a,tag:e,props:{},children:[{type:a,tag:r,props:{alt:s,src:"\u002Fcontent\u002Fimages\u002FAdvML101.PNG"},children:[]}]},{type:b,value:c},{type:a,tag:e,props:{},children:[{type:b,value:"With this in mind, we can jump into the attacks on ML systems."}]},{type:b,value:c},{type:a,tag:t,props:{id:z},children:[{type:a,tag:g,props:{href:"#machine-learning-attacks",ariaHidden:h,tabIndex:i},children:[{type:a,tag:j,props:{className:[k,l]},children:[]}]},{type:b,value:A}]},{type:b,value:c},{type:a,tag:e,props:{},children:[{type:b,value:"Attacks on machine learning systems can be categorized as follows:"}]},{type:b,value:"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"},{type:a,tag:"table",props:{},children:[{type:a,tag:"thead",props:{},children:[{type:a,tag:m,props:{},children:[{type:a,tag:u,props:{align:f},children:[{type:b,value:"Attack"}]},{type:a,tag:u,props:{align:f},children:[{type:b,value:"Overview"}]},{type:a,tag:u,props:{align:n},children:[{type:b,value:"Type"}]}]}]},{type:a,tag:"tbody",props:{},children:[{type:a,tag:m,props:{},children:[{type:a,tag:d,props:{align:f},children:[{type:b,value:"Model Evasion"}]},{type:a,tag:d,props:{align:f},children:[{type:b,value:"Attacker modifies a query in order to get a desired outcome. These attacks are performed by iteratively querying a model and observing the output."}]},{type:a,tag:d,props:{align:n},children:[{type:b,value:v}]}]},{type:a,tag:m,props:{},children:[{type:a,tag:d,props:{align:f},children:[{type:b,value:"Functional Extraction"}]},{type:a,tag:d,props:{align:f},children:[{type:b,value:"Attacker is able to recover a functionally equivalent model by iteratively querying the model. This allows an attacker to examine the offline copy of the model before further attacking the online model."}]},{type:a,tag:d,props:{align:n},children:[{type:b,value:v}]}]},{type:a,tag:m,props:{},children:[{type:a,tag:d,props:{align:f},children:[{type:b,value:"Model Poisoning"}]},{type:a,tag:d,props:{align:f},children:[{type:b,value:"Attacker contaminates the training data of an ML system in order to get a desired outcome at inference time. With influence over training data an attacker can create \"backdoors\" where an arbitrary input will result in a particular output. The model could be \"reprogrammed\" to perform a new undesired task. Further, access to training data would allow the attacker to create an offline model and create a Model Evasion. Access to training data could also result in the compromise of private data."}]},{type:a,tag:d,props:{align:n},children:[{type:b,value:"Train"}]}]},{type:a,tag:m,props:{},children:[{type:a,tag:d,props:{align:f},children:[{type:b,value:"Model Inversion"}]},{type:a,tag:d,props:{align:f},children:[{type:b,value:"Attacker recovers the features used to train the model. A successful attack would result in an attacker being able to launch a Membership inference attack. This attack could result in compromise of private data."}]},{type:a,tag:d,props:{align:n},children:[{type:b,value:v}]}]},{type:a,tag:m,props:{},children:[{type:a,tag:d,props:{align:f},children:[{type:b,value:"Traditional Attacks"}]},{type:a,tag:d,props:{align:f},children:[{type:b,value:"Attacker uses well established TTPs to attain their goal."}]},{type:a,tag:d,props:{align:n},children:[{type:b,value:"Both"}]}]}]}]},{type:b,value:c},{type:a,tag:t,props:{id:B},children:[{type:a,tag:g,props:{href:"#attack-scenarios",ariaHidden:h,tabIndex:i},children:[{type:a,tag:j,props:{className:[k,l]},children:[]}]},{type:b,value:C}]},{type:b,value:c},{type:a,tag:q,props:{id:D},children:[{type:a,tag:g,props:{href:"#attack-scenario-1-inference-attack",ariaHidden:h,tabIndex:i},children:[{type:a,tag:j,props:{className:[k,l]},children:[]}]},{type:b,value:E}]},{type:b,value:c},{type:a,tag:e,props:{},children:[{type:b,value:"Consider the most common deployment scenario where a model is deployed as\nan API endpoint. In this blackbox setting an attacker can only query the\nmodel and observe the response. The attacker controls the input to the\nmodel, but the attacker does not know how it is processed."}]},{type:b,value:c},{type:a,tag:e,props:{},children:[{type:a,tag:r,props:{alt:s,src:"\u002Fcontent\u002Fimages\u002FAdvML101_Inference.PNG"},children:[]}]},{type:b,value:c},{type:a,tag:"hr",props:{},children:[]},{type:b,value:c},{type:a,tag:q,props:{id:F},children:[{type:a,tag:g,props:{href:"#attack-scenario-2-training-time-attack",ariaHidden:h,tabIndex:i},children:[{type:a,tag:j,props:{className:[k,l]},children:[]}]},{type:b,value:G}]},{type:b,value:c},{type:a,tag:e,props:{},children:[{type:b,value:"Consider that an attacker has control over training data. This flavor of\nattack is shown in "},{type:a,tag:N,props:{to:"\u002Fstudies\u002FAML.CS0009"},children:[{type:b,value:"Tay Poisoning case study"}]},{type:b,value:" where the attacker was able to\ncompromise the training data via the feedback mechanism."}]},{type:b,value:c},{type:a,tag:e,props:{},children:[{type:a,tag:r,props:{alt:s,src:"\u002Fcontent\u002Fimages\u002FAdvML101_Traintime.PNG"},children:[]}]},{type:b,value:c},{type:a,tag:q,props:{id:H},children:[{type:a,tag:g,props:{href:"#attack-scenario-3-attack-on-edgeclient",ariaHidden:h,tabIndex:i},children:[{type:a,tag:j,props:{className:[k,l]},children:[]}]},{type:b,value:I}]},{type:b,value:c},{type:a,tag:e,props:{},children:[{type:b,value:"Consider that a model exists on a client (like a phone) or on the edge\n(such as IoT) . An attacker might have access to model code through\nreversing the service on the client. This flavor of attack is shown in\nBosch Case Study with EdgeAI."}]},{type:b,value:c},{type:a,tag:e,props:{},children:[{type:a,tag:r,props:{alt:s,src:"\u002Fcontent\u002Fimages\u002FAdvML101_Client.PNG"},children:[]}]},{type:b,value:c},{type:a,tag:t,props:{id:J},children:[{type:a,tag:g,props:{href:"#important-notes",ariaHidden:h,tabIndex:i},children:[{type:a,tag:j,props:{className:[k,l]},children:[]}]},{type:b,value:K}]},{type:b,value:c},{type:a,tag:"ol",props:{},children:[{type:b,value:c},{type:a,tag:w,props:{},children:[{type:b,value:"This does not cover all kinds of attacks -- adversarial ML is an active area of research with new classes of attacks constantly being discovered."}]},{type:b,value:c},{type:a,tag:w,props:{},children:[{type:b,value:"Though the illustration shows black-box attacks, these attacks have also been shown to work in white-box (where the attacker has access to either model architecture, code or training data)  settings."}]},{type:b,value:c},{type:a,tag:w,props:{},children:[{type:b,value:"Though we were not specific about what kind of data – image, audio, time series, or tabular data - research has shown that of these attacks are data agnostic."}]},{type:b,value:c}]},{type:b,value:c},{type:a,tag:t,props:{id:L},children:[{type:a,tag:g,props:{href:"#next-recommended-reading",ariaHidden:h,tabIndex:i},children:[{type:a,tag:j,props:{className:[k,l]},children:[]}]},{type:b,value:M}]},{type:b,value:c},{type:a,tag:e,props:{},children:[{type:b,value:"Head over to the "},{type:a,tag:N,props:{to:"\u002Fmatrices\u002FATLAS"},children:[{type:b,value:"ATLAS Matrix"}]},{type:b,value:" page to see a compendium of attacks in MITRE ATT&CK"},{type:a,tag:"sup",props:{},children:[{type:b,value:"®"}]},{type:b,value:" style."}]}]},dir:"\u002F",path:"\u002Fadversarial-ml-101",extension:".md",createdAt:O,updatedAt:O}}],fetch:{},mutations:[]}}("element","text","\n","td","p","left","a","true",-1,"span","icon","icon-link","tr","center",3,2,"h3","img","Adversarial ML 101","h2","th","Inference","li","train-time-vs-inference-time","Train time vs Inference time","machine-learning-attacks","Machine Learning Attacks","attack-scenarios","Attack Scenarios","attack-scenario-1-inference-attack","Attack Scenario #1: Inference Attack","attack-scenario-2-training-time-attack","Attack Scenario #2: Training Time Attack","attack-scenario-3-attack-on-edgeclient","Attack Scenario #3: Attack on Edge\u002FClient","important-notes","Important Notes","next-recommended-reading","Next Recommended Reading","nuxt-link","2023-08-14T16:39:24.693Z")));